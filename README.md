# comp560-s25-project
## Abstract
This paper investigates the efficacy of different vectorizations on the performance of multi-class classification with traditional machine learning and deep learning methods (logistic regression, support vector machines, and multilayer perceptrons) on classifying Reddit posts into subreddits and compares them to the performance of popular large language models on the same task. We employ Mike Swarbrick Jones’ “reddit self-post classification task” dataset reduced to 20 subreddits. First, we clean the data to prepare for bag-of-words and TF-IDF (CountVectorizer() and Tfidf() from scikit-learn) by removing stop words and punctuation, making all text lowercase, and tokenization. Then, we compute embeddings using models from Hugging Face, including all-MiniLM-L6-v2. We assess logistic regression and the support vector machine using TF-IDF and bag-of-words and the multilayer perceptron with all methods through accuracy and F1 score. Finally, we prompt various popular LLMs to classify the same data and calculate the accuracies and F1 scores. This work has applications in content moderation through ensuring content consistency and in recommendation systems by identifying similar content across subreddits on Reddit and social media platforms as a whole. As the popularity of integrating LLMs into content detection and sentiment analysis among businesses, determining its efficacy across use cases become pivotal to its implementation. 

## Dataset
https://www.kaggle.com/datasets/mswarbrickjones/reddit-selfposts?resource=download
